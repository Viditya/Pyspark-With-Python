{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa65084-2564-4b4f-af16-de1d5da5c69d",
   "metadata": {},
   "source": [
    "### Pyspark Part 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974fbe94-a230-4148-ab99-04536c15ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023cc092-1ebc-4e09-8302-1864a910de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b8c100-14c0-4012-9926-8c9c7ccfd592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/31 10:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/31 10:11:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('PractiseSpark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba62994-3b3b-4b67-8175-d52f26cca7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://instance-2.asia-south1-a.c.enhanced-bebop-393009.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PractiseSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc2efe19310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64679668-4625-4424-8042-e3cd048ebf3e",
   "metadata": {},
   "source": [
    "## Trying different ways of reading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a11ead4-fcf2-49b0-9ba6-dddf16ef3020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('../test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848c92f4-cd85-4ee0-860d-f8e34816ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|      _c0|_c1|       _c2|   _c3|\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210b5c04-d511-4e73-a8e9-8dab96c02ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('../test1.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5747b6ae-244c-4c25-bccb-4accabfb3d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2e055b5-51d2-4053-8a53-30797f3db676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('../test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92ce53c-12ab-4380-808b-ab71febb012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b42de1-705c-4d34-8697-dba5c0148cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0190b0d9-8533-44f8-848f-eaca0d937e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d0c78c3-8c83-4ae9-8e57-c831a9a6acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').option('inferschema','true').csv('../test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abf5e209-7e18-47c7-b2f8-d5ec831d2278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "869e062d-2e73-43c2-8ad4-2d68289dc08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10dd2771-9397-47bf-8ee8-e75498de65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('../test1.csv', header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a2849a7-49eb-4f71-a102-f532dbad0ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab364436-e60a-43d3-a5d1-88539ffe1ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5883aaa0-c202-4d11-bc2f-93bedee4900f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c8a9ba7-095c-4f4e-8e78-73ff7f0361b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', age=31, Experience=10, Salary=30000),\n",
       " Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000),\n",
       " Row(Name='Sunny', age=29, Experience=4, Salary=20000)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57595446-f69a-41fe-8c1e-589dc2a3526b",
   "metadata": {},
   "source": [
    "## Pyspark Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74478ea-72fd-45f9-b06a-9c9eb5febace",
   "metadata": {},
   "source": [
    "### Selecting columns from spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ca16db0-72e0-41a6-888b-47a130b09dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|age|\n",
      "+---------+---+\n",
      "|    Krish| 31|\n",
      "|Sudhanshu| 30|\n",
      "|    Sunny| 29|\n",
      "|     Paul| 24|\n",
      "|   Harsha| 21|\n",
      "|  Shubham| 23|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddb411be-db25-41d4-ac98-0b102aa7f458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Experience'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113748ac-af62-4a6e-8628-c9048e0e4829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c5ec3a0-b2fc-4c25-bcfd-8c5034d7cf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/31 10:29:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+------------------+\n",
      "|summary|  Name|               age|       Experience|            Salary|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "|  count|     6|                 6|                6|                 6|\n",
      "|   mean|  null|26.333333333333332|4.666666666666667|21333.333333333332|\n",
      "| stddev|  null| 4.179314138308661|3.559026084010437| 5354.126134736337|\n",
      "|    min|Harsha|                21|                1|             15000|\n",
      "|    max| Sunny|                31|               10|             30000|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c7f3c-2b1e-4087-b1fb-9e8543c61c26",
   "metadata": {},
   "source": [
    "### Adding columns in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ae0771b-cdd3-409d-bce5-1d3b76f36e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+------------------------+\n",
      "|     Name|age|Experience|Salary|Experience after 2 years|\n",
      "+---------+---+----------+------+------------------------+\n",
      "|    Krish| 31|        10| 30000|                      12|\n",
      "|Sudhanshu| 30|         8| 25000|                      10|\n",
      "|    Sunny| 29|         4| 20000|                       6|\n",
      "|     Paul| 24|         3| 20000|                       5|\n",
      "|   Harsha| 21|         1| 15000|                       3|\n",
      "|  Shubham| 23|         2| 18000|                       4|\n",
      "+---------+---+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumn('Experience after 2 years', df_pyspark['Experience'] + 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ced22922-ec64-4c51-81c1-51ac7e4e6832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+------------------------+--------------------+\n",
      "|     Name|age|Experience|Salary|Experience after 2 years|Salary after 2 years|\n",
      "+---------+---+----------+------+------------------------+--------------------+\n",
      "|    Krish| 31|        10| 30000|                      12|             36000.0|\n",
      "|Sudhanshu| 30|         8| 25000|                      10|             30000.0|\n",
      "|    Sunny| 29|         4| 20000|                       6|             24000.0|\n",
      "|     Paul| 24|         3| 20000|                       5|             24000.0|\n",
      "|   Harsha| 21|         1| 15000|                       3|             18000.0|\n",
      "|  Shubham| 23|         2| 18000|                       4|             21600.0|\n",
      "+---------+---+----------+------+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumns({'Experience after 2 years': df_pyspark['Experience']+2, 'Salary after 2 years': df_pyspark['Salary']*1.2}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12341edd-eedd-4df6-82a3-868f090fe6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Naam|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Name', 'Naam').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a03fb0-a496-4cb0-a34d-bc5ca445f7f8",
   "metadata": {},
   "source": [
    "## Pyspark Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e16559a-2029-4aa5-be38-d7dd27157ff0",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5edb308-be91-4f20-81a3-4a2fce11bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.options(header ='true',inferschema = 'true').csv('../test2.csv')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14ab7256-d176-4e54-9f44-ea4055b91e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d20b99d-461d-4e64-9b8c-00a2682a9db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.options(inferschema = 'true', header = 'true').csv('../test2.csv')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e423fbcb-e915-4962-b685-a18a6addec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6734e1f-4ab6-4fbb-93cb-5cc600ad11dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('../test2.csv', inferSchema = 'true', header = 'true')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4cf5528-d7a1-428f-ae05-9865b51c8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79288077-2a95-4862-905b-c90ffabdb948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|     Name|Experience|Salary|\n",
      "+---------+----------+------+\n",
      "|    Krish|        10| 30000|\n",
      "|Sudhanshu|         8| 25000|\n",
      "|    Sunny|         4| 20000|\n",
      "|     Paul|         3| 20000|\n",
      "|   Harsha|         1| 15000|\n",
      "|  Shubham|         2| 18000|\n",
      "|   Mahesh|      null| 40000|\n",
      "|     null|        10| 38000|\n",
      "|     null|      null|  null|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e85213b-8e8b-49df-ba31-633da8fb2daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77c511f9-04e3-4c17-8f4d-93912b3bd11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c5969e6-bf60-4432-ae3c-5edefa80bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a912c9ca-5f18-458f-ba8a-b3266f890504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     null| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', thresh = 3).show() ## For cases where atleast 3 non null values are there will be reatained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ade9bfea-f438-4911-9d35-bccf697a7ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', thresh = 2).show() ## For cases where atleast 2 non null values are there will be reatained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd516c22-1efc-487d-b290-f133c4b80464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', thresh = 1).show() ## For cases where atleast 1 non null values are there will be reatained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e48e6ef4-5d38-4530-bee2-60ea2f0f3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     null| 34|        10| 38000|\n",
      "|     null| 36|      null|  null|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', subset = 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45545ef5-93b8-4b85-b30a-20286a9e801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', subset = ['age', 'Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99eb3bfd-e744-4bac-a511-7d5455200034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     null| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'any', subset = ['age', 'Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ed83c3a-a3bb-4447-a6d4-f7653dcc0c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Krish|  31|        10| 30000|\n",
      "|     Sudhanshu|  30|         8| 25000|\n",
      "|         Sunny|  29|         4| 20000|\n",
      "|          Paul|  24|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|null|      null| 40000|\n",
      "|Missing values|  34|        10| 38000|\n",
      "|Missing values|  36|      null|  null|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing values',subset='Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf810949-80f8-4b9a-a5f0-71e6bb51d95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|         0| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|         0|     0|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(0, subset = ['salary','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3973e35-4c6a-4c24-8fce-28ad93c418b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|         0| 40000|\n",
      "|  Missing|  34|        10| 38000|\n",
      "|  Missing|  36|         0|     0|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing').na.fill(0, subset = ['salary','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62577b5d-f381-4883-9dcd-8b5a812f674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh|  0|         0| 40000|\n",
      "|  Missing| 34|        10| 38000|\n",
      "|  Missing| 36|         0|     0|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing').na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0bdf5300-d3a7-40ef-bdfb-da38ce4f1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|         0| 40000|\n",
      "|  Missing|  34|        10| 38000|\n",
      "|  Missing|  36|         0|     0|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing').na.fill(0,['salary','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c6fdfa5-15a7-4930-a42a-e3306bf01449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|         0| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|         0|     0|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(0,['Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b65430c-e355-4c6f-846a-9c4d5506a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('../test2.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b0d4015-1e27-4df0-adfe-4f01f66daeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+--------------+--------------+\n",
      "|     Name| age|    Experience|        Salary|\n",
      "+---------+----+--------------+--------------+\n",
      "|    Krish|  31|            10|         30000|\n",
      "|Sudhanshu|  30|             8|         25000|\n",
      "|    Sunny|  29|             4|         20000|\n",
      "|     Paul|  24|             3|         20000|\n",
      "|   Harsha|  21|             1|         15000|\n",
      "|  Shubham|  23|             2|         18000|\n",
      "|   Mahesh|null|Missing values|         40000|\n",
      "|     null|  34|            10|         38000|\n",
      "|     null|  36|Missing values|Missing values|\n",
      "+---------+----+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing values',['Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f9f06-d17d-4f89-9f13-7115f62853fd",
   "metadata": {},
   "source": [
    "### Using imputer to impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e8dce2f-a7a5-4585-95b0-f0205c044f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('inferschema', 'true').csv('../test2.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f80ebcd7-e474-471c-89ba-e0cd451142e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b5a2336c-f727-40eb-b045-39b308374ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols= ['age','Experience','Salary'],\n",
    "    outputCols= ['{}_col'.format(c) for c in ['age','Experience','Salary']],\n",
    "    strategy= 'mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d3600a1-9df1-4464-b6f6-109f3a32c6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "|     Name| age|Experience|Salary|age_col|Experience_col|Salary_col|\n",
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "|    Krish|  31|        10| 30000|     31|            10|     30000|\n",
      "|Sudhanshu|  30|         8| 25000|     30|             8|     25000|\n",
      "|    Sunny|  29|         4| 20000|     29|             4|     20000|\n",
      "|     Paul|  24|         3| 20000|     24|             3|     20000|\n",
      "|   Harsha|  21|         1| 15000|     21|             1|     15000|\n",
      "|  Shubham|  23|         2| 18000|     23|             2|     18000|\n",
      "|   Mahesh|null|      null| 40000|     28|             5|     40000|\n",
      "|     null|  34|        10| 38000|     34|            10|     38000|\n",
      "|     null|  36|      null|  null|     36|             5|     25750|\n",
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76cda029-f4cc-4821-883d-776df558708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+------------------+-----------------+\n",
      "|summary|  Name|               age|        Experience|           Salary|\n",
      "+-------+------+------------------+------------------+-----------------+\n",
      "|  count|     7|                 8|                 7|                8|\n",
      "|   mean|  null|              28.5| 5.428571428571429|          25750.0|\n",
      "| stddev|  null|5.3718844791323335|3.8234863173611093|9361.776388210581|\n",
      "|    min|Harsha|                21|                 1|            15000|\n",
      "|    max| Sunny|                36|                10|            40000|\n",
      "+-------+------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a89a2cef-d189-4e3f-b9c6-96c2289f0a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 8|\n",
      "|   mean|              28.5|\n",
      "| stddev|5.3718844791323335|\n",
      "|    min|                21|\n",
      "|    max|                36|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "afdb4c8d-892e-4f38-ad28-d3614debb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols= ['age','Experience','Salary'],\n",
    "    outputCols= ['{}_col'.format(c) for c in ['age','Experience','Salary']]\n",
    ").setStrategy('median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "68d5e32c-73dc-4b16-8215-403ef64fde9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "|     Name| age|Experience|Salary|age_col|Experience_col|Salary_col|\n",
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "|    Krish|  31|        10| 30000|     31|            10|     30000|\n",
      "|Sudhanshu|  30|         8| 25000|     30|             8|     25000|\n",
      "|    Sunny|  29|         4| 20000|     29|             4|     20000|\n",
      "|     Paul|  24|         3| 20000|     24|             3|     20000|\n",
      "|   Harsha|  21|         1| 15000|     21|             1|     15000|\n",
      "|  Shubham|  23|         2| 18000|     23|             2|     18000|\n",
      "|   Mahesh|null|      null| 40000|     29|             4|     40000|\n",
      "|     null|  34|        10| 38000|     34|            10|     38000|\n",
      "|     null|  36|      null|  null|     36|             4|     20000|\n",
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187f246-6df3-4e69-ad81-a179a46b8345",
   "metadata": {},
   "source": [
    "## Pyspark Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995bf96-95ae-4d1f-b6cb-3645c6a99282",
   "metadata": {},
   "source": [
    "### Filter operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "77c4e979-08d1-431d-ae0d-db3043bc071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('../test1.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e6725cd2-a231-4ed1-b70f-a8547260936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4e86c89d-c342-4483-92e8-f6ad36b6df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Experience']< 10 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bdfbade3-2a2f-4bf2-84ae-6241a2b3d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('Experience < 10' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d4e41a5-9b3b-450e-bbc0-fc56a22867f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|age|\n",
      "+---------+---+\n",
      "|    Krish| 31|\n",
      "|Sudhanshu| 30|\n",
      "|    Sunny| 29|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('Experience > 3').select(['Name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6d33923b-6abc-42bb-83f5-0d25687fabc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Sunny| 29|         4| 20000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Experience'] > 3) & (df_pyspark['age'] < 30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "12b936b4-93a9-4a04-84a0-84f5e02fc5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Experience'] > 3) | (df_pyspark['age'] < 30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fdb9bdc4-84ba-4667-a2ea-5ce5e6edaf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Experience'] > 3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4f528-b682-4e48-a6d0-95b738f737e5",
   "metadata": {},
   "source": [
    "## Pyspark Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6dbfc7-a446-4128-86c7-e0f8695969d7",
   "metadata": {},
   "source": [
    "### Group by and aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "926534b9-c660-4723-a97f-d908a8f90c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('../test3.csv', inferSchema=True, header = True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aa71263a-527e-4a15-a1d4-dde2af52b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b8c8bf12-db0b-4dfa-b917-b7a1449b4459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1893b1ba-e82b-4b6a-b2e3-871cda098b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ed9ad1d9-19ab-4b6c-a3d1-b618b496b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3b27e9c9-dc62-43a0-9991-8010b1fb42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     Name|count|\n",
      "+---------+-----+\n",
      "|Sudhanshu|    3|\n",
      "|    Sunny|    2|\n",
      "|    Krish|    3|\n",
      "|   Mahesh|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ddaf6252-d58d-4fab-91e8-19f8e03e60f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "798e41db-768c-4b0a-b373-1097c24ef242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e88d1ebe-8a43-46ee-9090-f65857520382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a66b3970-4595-476f-a132-8f50b1524ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|max(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      10000|\n",
      "|    Big Data|       5000|\n",
      "|Data Science|      20000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4467bc57-75d2-4ee3-85db-8246af61f1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|max(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      20000|\n",
      "|    Sunny|      10000|\n",
      "|    Krish|      10000|\n",
      "|   Mahesh|       4000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bd83c2d0-5458-46a5-ba45-daf2ca4b66d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "765885d9-312a-438c-80c3-773be15e9103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "034bce50-deb2-4bd6-8c72-95533d654aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|median(Salary)|\n",
      "+--------------+\n",
      "|        5000.0|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'median'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3ff5ee5a-cf29-4b5a-aaaa-a1a241ea8dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|    Krish|Data Science| 10000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.sort('Salary', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "52b16c81-f07b-41b8-ab4f-6bc51e3fb18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   stddev(Salary)|\n",
      "+-----------------+\n",
      "|5396.500923952689|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'stddev'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2b5b956f-566b-42ee-8fa3-692d3aa1fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(Salary)|\n",
      "+-----------+\n",
      "|     7300.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aa69eae0-67b0-4c47-8ec7-da8366b433bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+-----------------+\n",
      "|summary| Name|Departments|           salary|\n",
      "+-------+-----+-----------+-----------------+\n",
      "|  count|   10|         10|               10|\n",
      "|   mean| null|       null|           7300.0|\n",
      "| stddev| null|       null|5396.500923952689|\n",
      "|    min|Krish|   Big Data|             2000|\n",
      "|    max|Sunny|        IOT|            20000|\n",
      "+-------+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace03e6-9b50-4db0-a469-8ef3bcaedcaf",
   "metadata": {},
   "source": [
    "## Part 6 Pyspark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6e9e4-8705-4654-a53a-36dba73679ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
